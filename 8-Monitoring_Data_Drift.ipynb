{"cells":[{"cell_type":"markdown","source":["# Monitoring Data Drift\n","\n","Over time, models can become less effective at predicting accurately due to changing trends in feature data. This phenomenon is known as *data drift*, and it's important to monitor your machine learning solution to detect it so you can retrain your models if necessary.\n","\n","In this lab, you'll configure data drift monitoring for datasets.\n","\n","## Install the DataDriftDetector module\n","\n","To define a data drift monitor, you'll need to ensure that you have the latest version of the Azure ML SDK installed, and install the **datadrift** module; so run the following cell to do that:"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["!pip install --upgrade azureml-sdk[notebooks,automl,explain]\n","!pip install --upgrade azureml-datadrift\n","# Restart the kernel after installation is complete!"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1600469823838}}},{"cell_type":"markdown","source":["> **Important**: Now you'll need to <u>restart the kernel</u>. In Jupyter, on the **Kernel** menu, select **Restart and Clear Output**. Then, when the output from the cell above has been removed and the kernel is restarted, continue the steps below.\n","\n","## Connect to Your Workspace\n","\n","The first thing you need to do is to connect to your workspace using the Azure ML SDK.\n","\n","> **Note**: You may be prompted to authenticate. Just copy the code and click the link provided to sign into your Azure subscription, and then return to this notebook."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["from azureml.core import Workspace\n","\n","# Load the workspace from the saved config file\n","ws = Workspace.from_config()\n","print('Ready to work with', ws.name)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1600469824738}}},{"cell_type":"markdown","source":["## Create a Baseline Dataset\n","\n","To monitor a dataset for data drift, you must register a *baseline* dataset (usually the dataset used to train your model) to use as a point of comparison with data collected in the future. \n","\n","In this case we will use the already existing **flight_delays** dataset as our baseline dataset"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["# Get the training dataset\n","baseline_dataset = ws.datasets.get('flight_delays_data')"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1600469824913}}},{"cell_type":"markdown","source":["## Create a Target Dataset\n","\n","Over time, you can collect new data with the same features as your baseline training data. To compare this new data to the baseline data, you must define a target dataset that includes the features you want to analyze for data drift as well as a timestamp field that indicates the point in time when the new data was current -this enables you to measure data drift over temporal intervals. The timestamp can either be a field in the dataset itself, or derived from the folder and filename pattern used to store the data. For example, you might store new data in a folder hierarchy that consists of a folder for the year, containing a folder for the month, which in turn contains a folder for the day; or you might just encode the year, month, and day in the file name like this: *data_2020-01-29.csv*; which is the approach taken in the following code:"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["import datetime as dt\n","from azureml.core import Dataset\n","import os\n","\n","data_folder = 'data'\n","os.makedirs(data_folder, exist_ok=True)\n","\n","print(data_folder, 'Folder ready.')\n","\n","default_ds = ws.get_default_datastore()\n","\n","print('Generating simulated data...')\n","\n","\n","\n","# Load 10 percentage of the baseline data for drift simulation\n","drift_data = baseline_dataset.random_split(percentage=0.1, seed=123)[0]\n","drift_data = drift_data.to_pandas_dataframe()\n","\n","file_path = 'data/flight_delays.csv'\n","drift_data.head().to_csv(file_path)\n","\n","\n","# We'll generate data for the past 6 weeks\n","weeknos = reversed(range(6))\n","\n","file_paths = []\n","for weekno in weeknos:\n","    \n","    # Get the date X weeks ago\n","    data_date = dt.date.today() - dt.timedelta(weeks=weekno)\n","    \n","    # Modify data to ceate some drift\n","    drift_data['Month'] = drift_data['Month'] + 1\n","    drift_data['CRSDepTime'] = drift_data['CRSDepTime'] + 123\n","    drift_data['CRSArrTime'] = drift_data['CRSArrTime'] + 325\n","    drift_data['DepDelay'] = drift_data['DepDelay'] * 1.1\n","    \n","    # Save the file with the date encoded in the filename\n","    file_path = 'data/flight_delays_{}.csv'.format(data_date.strftime(\"%Y-%m-%d\"))\n","    drift_data.to_csv(file_path)\n","    file_paths.append(file_path)\n","\n","# Upload the files\n","path_on_datastore = 'flight_delays_target'\n","default_ds.upload_files(files=file_paths,\n","                       target_path=path_on_datastore,\n","                       overwrite=True,\n","                       show_progress=True)\n","\n","# Use the folder partition format to define a dataset with a 'date' timestamp column\n","partition_format = path_on_datastore + '/flight_delays_{date:yyyy-MM-dd}.csv'\n","target_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, path_on_datastore + '/*.csv'),\n","                                                       partition_format=partition_format)\n","\n","# Register the target dataset\n","print('Registering target dataset...')\n","target_data_set = target_data_set.with_timestamp_columns('date').register(workspace=ws,\n","                                                                          name='Flight Delays target',\n","                                                                          description='Flight Delays target data',\n","                                                                          tags = {'format':'CSV'},\n","                                                                          create_new_version=True)\n","\n","print('Target dataset registered!')"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1600469858612}}},{"cell_type":"markdown","source":["## Create a Data Drift Monitor\n","\n","Now you're ready to create a data drift monitor for the flight delays data. The data drift monitor will run periodicaly or on-demand to compare the baseline dataset with the target dataset, to which new data will be added over time.\n","\n","### Create a Compute Target\n","\n","To run the data drift monitor, you'll need a compute target. create an Azure Machine Learning compute cluster in your workspace (or use an existing one if you have created it previously).\n","\n","> **Important**: Change *your-compute-cluster* to the unique name for your compute cluster in the code below before running it!"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["from azureml.core.compute import ComputeTarget, AmlCompute\n","from azureml.core.compute_target import ComputeTargetException\n","\n","cluster_name = \"aml-cluster\"\n","\n","try:\n","    # Get the cluster if it exists\n","    training_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n","    print('Found existing cluster, use it.')\n","except ComputeTargetException:\n","    # If not, create it\n","    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS2_V2', max_nodes=2)\n","    training_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n","\n","training_cluster.wait_for_completion(show_output=True)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1600469858875}}},{"cell_type":"markdown","source":["### Define the Data Drift Monitor\n","\n","Now you're ready to use a **DataDriftDetector** class to define the data drift monitor for your data. You can specify the features you want to monitor for data drift, the name of the compute target to be used to run the monitoring process, the frequency at which the data should be compared, the data drift threshold above which an alert should be triggered, and the latency (in hours) to allow for data collection."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["from azureml.datadrift import DataDriftDetector\n","\n","# set up feature list\n","features = ['Month', 'CRSDepTime', 'CRSArrTime', 'DepDelay']\n","\n","# set up data drift detector\n","monitor = DataDriftDetector.create_from_datasets(ws, 'flight-delays-drift-detector', baseline_dataset, target_data_set,\n","                                                      compute_target=cluster_name, \n","                                                      frequency='Week', \n","                                                      feature_list=features, \n","                                                      drift_threshold=.3, \n","                                                      latency=24)\n","monitor"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1600469860041}}},{"cell_type":"markdown","source":["## Backfill the Monitor\n","\n","You have a baseline dataset and a target dataset that includes simulated weekly data collection for six weeks. You can use this to backfill the monitor so that it can analyze data drift between the original baseline and the target data.\n","\n","> **Note** This may take some time to run, as the compute target must be started to run the backfill analysis. The widget may not always update to show the status, so click the link to observe the experiment status in Azure Machine Learning studio!"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["from azureml.widgets import RunDetails\n","\n","backfill = monitor.backfill( dt.datetime.now() - dt.timedelta(weeks=6), dt.datetime.now())\n","\n","RunDetails(backfill).show()\n","backfill.wait_for_completion()"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"markdown","source":["## Analyze Data Drift\n","\n","You can use the following code to examine data drift for the points in time collected in the backfill run."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["drift_metrics = backfill.get_metrics()\n","for metric in drift_metrics:\n","    print(metric, drift_metrics[metric])"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"markdown","source":["You can also visualize the data drift metrics in [Azure Machine Learning studio](https://ml.azure.com) by following these steps:\n","\n","1. On the **Datasets** page, view the **Dataset monitors** tab.\n","2. Click the data drift monitor you want to view.\n","3. Select the date range over which you want to view data drift metrics (if the column chart does not show multiple weeks of data, wait a minute or so and click **Refresh**).\n","4. Examine the charts in the **Drift overview** section at the top, which show overall drift magnitude and the drift contribution per feature.\n","5. Explore the charts in the **Feature detail** section at the bottom, which enable you to see various measures of drift for individual features.\n","\n","> **Note**: For help understanding the data drift metrics, see the [How to monitor datasets](https://docs.microsoft.com/azure/machine-learning/how-to-monitor-datasets#understanding-data-drift-results) in the Azure Machine Learning documentation.\n","\n","## Explore Further\n","\n","This lab is designed to introduce you to the concepts and principles of data drift monitoring. To learn more about monitoring data drift using datasets, see the [Detect data drift on datasets](https://docs.microsoft.com/azure/machine-learning/how-to-monitor-datasets) in the Azure machine Learning documentation.\n","\n","You can also configure data drift monitoring for services deployed in an Azure Kubernetes Service (AKS) cluster. For more information about this, see [Detect data drift on models deployed to Azure Kubernetes Service (AKS)](https://docs.microsoft.com/azure/machine-learning/how-to-monitor-data-drift) in the Azure Machine Learning documentation.\n"],"metadata":{"nteract":{"transient":{"deleting":false}}}}],"metadata":{"kernelspec":{"name":"python3-azureml","language":"python","display_name":"Python 3.6 - AzureML"},"language_info":{"name":"python","version":"3.6.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernel_info":{"name":"python3-azureml"},"nteract":{"version":"nteract-front-end@1.0.0"}},"nbformat":4,"nbformat_minor":2}