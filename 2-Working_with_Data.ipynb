{"cells":[{"cell_type":"markdown","source":["# Sourcing and Transforming Data"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"markdown","source":["### Connect to workspace"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["import azureml.core\n","from azureml.core import Workspace\n","\n","# Load the workspace from the saved config file\n","\n","ws = Workspace.from_config()\n","print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1600433283536},"tags":[]}},{"cell_type":"markdown","source":["### Work with a Datastore\n","\n","In Azure ML, *datastores* are references to storage locations, such as Azure Storage blob containers. Every workspace has a default datastore - usually the Azure storage blob container that was created with the workspace. If you need to work with data that is stored in different locations, you can add custom datastores to your workspace and set any of them to be the default.\n","\n","### View Datastores\n","\n","Run the following code to determine the datastores in your workspace:"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["# Get the default datastore\n","default_ds = ws.get_default_datastore()\n","\n","# Enumerate all datastores, indicating which is the default\n","for ds_name in ws.datastores:\n","    print(ds_name, \"- Default =\", ds_name == default_ds.name)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1600433284367},"tags":[]}},{"source":["You can also view and manage datastores in your workspace on the Datastores page for your workspace in [Azure ML Studio](https://ml.azure.com).\n","\n","### Upload Data to a Datastore\n","\n","Now that you have determined the available datastores, you can upload files from your local file system to a datastore so that it will be accessible to experiments running in the workspace, regardless of where the experiment script is actually being run."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["default_ds.upload_files(files=['./data/flight_delays_data.csv'], # Upload the diabetes csv files in /data\n","                       target_path='data/', # Put it in a folder path in the datastore\n","                       overwrite=True, # Replace existing files of the same name\n","                       show_progress=True)"]},{"cell_type":"markdown","source":["## Work with Datasets\n","\n","While you can read data directly from datastores, Azure Machine Learning provides a further abstraction for data in the form of *datasets*. A dataset is a versioned reference to a specific set of data that you may want to use in an experiment. Datasets can be *tabular* or *file*-based.\n","\n","### Create and Register Tabular Dataset\n","\n","Let's create a dataset from the flight delays data you uploaded to the datastore. In this case, the data is in a structured format in a CSV file, so we'll use a *tabular* dataset.\n","\n","\n","Once we create the datasets that reference the flight delays data, you can register it to make it easily accessible to any experiment being run in the workspace.\n","\n","We'll register the tabular dataset as **flight_delays_data**"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["from azureml.core import Dataset\n","\n","default_ds = ws.get_default_datastore()\n","\n","if 'flight_delays_data' not in ws.datasets:\n","    #Create a tabular dataset from the path on the datastore (this may take a short while)\n","    csv_path = [(default_ds, 'data/flight_delays_data.csv.csv')]\n","    tab_data_set = Dataset.Tabular.from_delimited_files(path=csv_path)\n","\n","    # Register the tabular dataset\n","    try:\n","        tab_data_set = tab_data_set.register(workspace=ws, \n","                                name='flight_delays_data',\n","                                description='flight delays data',\n","                                tags = {'format':'CSV'},\n","                                create_new_version=True)\n","        print('Dataset registered.')\n","    except Exception as ex:\n","        print(ex)\n","else:\n","    print('Dataset already registered.')"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1600433284444}}},{"cell_type":"markdown","source":["Get the flight_delays_data and display first 20 rows examing the content of the data"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["# Get the training dataset\n","dataset = ws.datasets.get('flight_delays_data')\n","dataset = dataset.to_pandas_dataframe()\n","dataset.head(20)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1600433290515}}},{"cell_type":"markdown","source":["let's do a quick description of the features available."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["dataset.describe()"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1600433292273}}},{"cell_type":"markdown","source":["Displaying a information of the dataset will help us know which columns need to be engineered."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["dataset.info()"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1600433292424}}},{"cell_type":"markdown","source":["### Feature Engineering\n","\n","Feature engineering here will include removing target leakers and features that are not useful to our hypothesis. \n","We will then make sure the columns(features) are of the right data types for the algorithm to be used for the prediction."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["# Get the training dataset\n","dataset = ws.datasets.get('flight_delays_data')\n","dataset = dataset.to_pandas_dataframe().dropna()\n","\n","# Remove target leaker and features that are not useful\n","target_leakers = ['DepDel15','ArrDelay','Cancelled','Year']\n","dataset.drop(columns=target_leakers, axis=1, inplace=True)\n","\n","# convert some columns to categorical features\n","columns_as_categorical = ['OriginAirportID','DestAirportID','ArrDel15']\n","dataset[columns_as_categorical] = dataset[columns_as_categorical].astype('object')\n","\n","# The labelEncoder and OneHotEncoder only works on categorical features. We need first to extract the categorial featuers using boolean mask.\n","categorical_feature_mask = dataset.dtypes == object \n","categorical_cols = dataset.columns[categorical_feature_mask].tolist()\n","categorical_cols"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1600433296809}}},{"cell_type":"markdown","source":["LabelEncoder converts each class under specified feature to a numerical value. \n","\n","Letâ€™s go through the steps to see how to do it."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","\n","le = LabelEncoder()\n","\n","# Apply LabelEncoder on each of the categorical columns:\n","dataset[categorical_cols] = dataset[categorical_cols].apply(lambda col:le.fit_transform(col))\n","dataset[categorical_cols].head(10)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1600433300687}}},{"cell_type":"code","source":["# Drop all null values\n","dataset.dropna(inplace=True)\n","dataset.head(20)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1600433301430}}},{"cell_type":"markdown","source":["Doing a relative data split based on the Month column"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["train_ds, test_ds = dataset.loc[dataset['Month'] < 9], dataset.loc[dataset['Month'] >= 9]\n","train_count = train_ds.Month.count()\n","test_count = test_ds.Month.count()\n","print('Test data ratio:',(test_count/(test_count+train_count))*100)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1600433406759}}},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}}],"metadata":{"kernelspec":{"name":"python_defaultSpec_1600507958321","language":"python","display_name":"Python 3.8.5 64-bit"},"language_info":{"name":"python","version":"3.8.5-final","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernel_info":{"name":"python3-azureml"},"nteract":{"version":"nteract-front-end@1.0.0"}},"nbformat":4,"nbformat_minor":2}